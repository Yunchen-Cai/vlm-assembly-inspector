import torch
import os
import sys
from pathlib import Path
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from peft import PeftModel

# ================= é…ç½®åŒºåŸŸ =================
# 1. è‡ªåŠ¨å®šä½è·¯å¾„
CURRENT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = CURRENT_DIR.parent
HF_CACHE = PROJECT_ROOT / "cache" / "huggingface"
os.environ["HF_HOME"] = str(HF_CACHE)

# æ¨¡å‹è·¯å¾„
BASE_MODEL_PATH = r"D:\vlm_assembly\cache\huggingface\hub\Qwen2.5-VL-7B-Instruct"
ADAPTER_PATH = PROJECT_ROOT / "output" / "v1_lora_baseline"

# å›¾ç‰‡é»˜è®¤æœç´¢è·¯å¾„ (æ ¹æ®æ‚¨çš„æè¿°)
# è„šæœ¬ä¼šè‡ªåŠ¨å» D:\vlm_assembly\data\temp_frames ä¸‹æ‰¾å›¾ç‰‡
DEFAULT_IMAGE_DIR = PROJECT_ROOT / "data" / "temp_frames"


# ===========================================

def load_model():
    """åªè¿è¡Œä¸€æ¬¡ï¼šåŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜"""
    print("\n" + "=" * 40)
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
    print(f"1. åŠ è½½åŸºåº§æ¨¡å‹: {BASE_MODEL_PATH}")

    # ä½¿ç”¨ 4bit åŠ è½½ï¼Œé€Ÿåº¦å¿«ä¸”çœæ˜¾å­˜
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        load_in_4bit=True,
        trust_remote_code=True
    )

    print(f"2. æŒ‚è½½ LoRA æƒé‡: {ADAPTER_PATH}")
    # åŠ è½½å¾®è°ƒåçš„æƒé‡
    try:
        model = PeftModel.from_pretrained(model, str(ADAPTER_PATH))
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: åŠ è½½ LoRA å¤±è´¥ï¼Œå°†ä½¿ç”¨çº¯åŸºåº§æ¨¡å‹ã€‚é”™è¯¯: {e}")

    print("3. åŠ è½½å¤„ç†å™¨...")
    processor = AutoProcessor.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    print("âœ… ç³»ç»Ÿå°±ç»ªï¼ç­‰å¾…è¾“å…¥...")
    print("=" * 40 + "\n")
    return model, processor


def run_inference_loop(model, processor):
    """è¿›å…¥æ— é™å¾ªç¯ï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥"""

    print(f"ğŸ“‚ é»˜è®¤å›¾ç‰‡ç›®å½•: {DEFAULT_IMAGE_DIR}")
    print("ğŸ’¡ æç¤º: è¾“å…¥æ–‡ä»¶åå³å¯ (ä¾‹å¦‚: 202512121915_step1_frame_2.jpg)")
    print("âŒ é€€å‡º: è¾“å…¥ 'exit' æˆ– 'q'")

    while True:
        # 1. è·å–ç”¨æˆ·è¾“å…¥
        filename = input("\n>>> è¯·è¾“å…¥å›¾ç‰‡æ–‡ä»¶å: ").strip()

        # é€€å‡ºæ¡ä»¶
        if filename.lower() in ['exit', 'quit', 'q', 'é€€å‡º']:
            print("ğŸ‘‹ å†è§ï¼")
            break

        if not filename:
            continue

        # 2. æ™ºèƒ½æ„å»ºè·¯å¾„
        # å¦‚æœç”¨æˆ·ç›´æ¥ç²˜è´´äº†ç»å¯¹è·¯å¾„ (D:\...)ï¼Œå°±ç”¨ç»å¯¹è·¯å¾„
        # å¦‚æœåªè¾“å…¥äº†æ–‡ä»¶åï¼Œå°±æ‹¼æ¥åˆ°é»˜è®¤ç›®å½•
        if os.path.isabs(filename):
            image_path = Path(filename)
        else:
            image_path = DEFAULT_IMAGE_DIR / filename

        # 3. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not image_path.exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ -> {image_path}")
            print(f"   è¯·ç¡®è®¤æ–‡ä»¶åœ¨ {DEFAULT_IMAGE_DIR} ä¸‹ï¼Œæˆ–è¾“å…¥ç»å¯¹è·¯å¾„ã€‚")
            continue

        try:
            # 4. åŠ è½½å›¾ç‰‡
            image = Image.open(image_path).convert("RGB")
            print(f"ğŸ“¸ æ­£åœ¨åˆ†æ: {image_path.name} ...")

            # 5. æ„é€  Prompt (å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)
            prompt_text = "You are an assembly quality inspector. Observe this frame. Output in this exact format: 'Description: [brief action description]' followed by JSON: {\"action\": \"...\", \"tool\": \"...\", \"part\": \"...\", \"phase\": \"...\"}. Judge based on standard assembly process."

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt_text},
                    ],
                }
            ]

            # 6. å¤„ç†è¾“å…¥
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(
                text=[text],
                images=[image],
                padding=True,
                return_tensors="pt",
            )
            inputs = inputs.to("cuda")

            # 7. æ¨¡å‹ç”Ÿæˆ
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.1,  # ä½æ¸©ï¼Œä¿è¯è¾“å‡ºç¨³å®š
                    top_p=0.9
                )

            # 8. è§£ç å¹¶è¾“å‡º
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]

            print("-" * 20 + " æ¨¡å‹è¾“å‡º " + "-" * 20)
            print(output_text)
            print("-" * 50)

        except Exception as e:
            print(f"âŒ æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")


if __name__ == "__main__":
    # 1. å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡
    model_instance, processor_instance = load_model()

    # 2. è¿›å…¥äº¤äº’å¾ªç¯
    run_inference_loop(model_instance, processor_instance)